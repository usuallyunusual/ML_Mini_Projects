\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}ML\_Prog4}{1}{section.1}\protected@file@percent }
\newlabel{ml_prog4}{{1}{1}{ML\_Prog4}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Demonstrating lda and pca}{1}{subsection.1.1}\protected@file@percent }
\newlabel{demonstrating-lda-and-pca}{{1.1}{1}{Demonstrating lda and pca}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Importing necessary libraries}{1}{subsection.1.2}\protected@file@percent }
\newlabel{importing-necessary-libraries}{{1.2}{1}{Importing necessary libraries}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Class map. Dict containing values of each class}{1}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{class-map.-dict-containing-values-of-each-class}{{1.2.1}{1}{Class map. Dict containing values of each class}{subsubsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Using fashion mnist dataset}{2}{subsection.1.3}\protected@file@percent }
\newlabel{using-fashion-mnist-dataset}{{1.3}{2}{Using fashion mnist dataset}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Has about 60000 train and 10000 test images rangin through 10 classes}{2}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{has-about-60000-train-and-10000-test-images-rangin-through-10-classes}{{1.3.1}{2}{Has about 60000 train and 10000 test images rangin through 10 classes}{subsubsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Use index to see through the dataset (Train)}{2}{subsubsection.1.3.2}\protected@file@percent }
\newlabel{use-index-to-see-through-the-dataset-train}{{1.3.2}{2}{Use index to see through the dataset (Train)}{subsubsection.1.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Class distribution of training set. Uniform}{3}{subsection.1.4}\protected@file@percent }
\newlabel{class-distribution-of-training-set.-uniform}{{1.4}{3}{Class distribution of training set. Uniform}{subsection.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Class distribution of test set. Also uniform}{3}{subsection.1.5}\protected@file@percent }
\newlabel{class-distribution-of-test-set.-also-uniform}{{1.5}{3}{Class distribution of test set. Also uniform}{subsection.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Class distirbutions help debuggin classifier issues and having correct assumptions about data}{3}{subsubsection.1.5.1}\protected@file@percent }
\newlabel{class-distirbutions-help-debuggin-classifier-issues-and-having-correct-assumptions-about-data}{{1.5.1}{3}{Class distirbutions help debuggin classifier issues and having correct assumptions about data}{subsubsection.1.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Normalizing data and converting to float}{4}{subsection.1.6}\protected@file@percent }
\newlabel{normalizing-data-and-converting-to-float}{{1.6}{4}{Normalizing data and converting to float}{subsection.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1}tf needs float values}{4}{subsubsection.1.6.1}\protected@file@percent }
\newlabel{tf-needs-float-values}{{1.6.1}{4}{tf needs float values}{subsubsection.1.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Applying PCA() on the dataset}{4}{subsection.1.7}\protected@file@percent }
\newlabel{applying-pca-on-the-dataset}{{1.7}{4}{Applying PCA() on the dataset}{subsection.1.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.1}PCA maps the input feature space into another feature space based on eigenvectors. It considers variance of data}{4}{subsubsection.1.7.1}\protected@file@percent }
\newlabel{pca-maps-the-input-feature-space-into-another-feature-space-based-on-eigenvectors.-it-considers-variance-of-data}{{1.7.1}{4}{PCA maps the input feature space into another feature space based on eigenvectors. It considers variance of data}{subsubsection.1.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.2}It is unsupervised and helps remove correlated attributes. Denoising can also be done.}{4}{subsubsection.1.7.2}\protected@file@percent }
\newlabel{it-is-unsupervised-and-helps-remove-correlated-attributes.-denoising-can-also-be-done.}{{1.7.2}{4}{It is unsupervised and helps remove correlated attributes. Denoising can also be done}{subsubsection.1.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.3}Has disadvantages. Showcased later}{4}{subsubsection.1.7.3}\protected@file@percent }
\newlabel{has-disadvantages.-showcased-later}{{1.7.3}{4}{Has disadvantages. Showcased later}{subsubsection.1.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.4}Displaying Correlation between a subset of input feature space}{4}{subsubsection.1.7.4}\protected@file@percent }
\newlabel{displaying-correlation-between-a-subset-of-input-feature-space}{{1.7.4}{4}{Displaying Correlation between a subset of input feature space}{subsubsection.1.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.5}Converting to PCA feature space}{5}{subsubsection.1.7.5}\protected@file@percent }
\newlabel{converting-to-pca-feature-space}{{1.7.5}{5}{Converting to PCA feature space}{subsubsection.1.7.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.6}Displaying correlation of pca feature space.}{5}{subsubsection.1.7.6}\protected@file@percent }
\newlabel{displaying-correlation-of-pca-feature-space.}{{1.7.6}{5}{Displaying correlation of pca feature space}{subsubsection.1.7.6}{}}
\newlabel{clearly-reduced.}{{1.7.6}{5}{Clearly reduced}{section*.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Clearly reduced.}{5}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Visualizing PCA vs LDA}{5}{subsection.1.8}\protected@file@percent }
\newlabel{visualizing-pca-vs-lda}{{1.8}{5}{Visualizing PCA vs LDA}{subsection.1.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.1}PCA feature space in 2D plot}{5}{subsubsection.1.8.1}\protected@file@percent }
\newlabel{pca-feature-space-in-2d-plot}{{1.8.1}{5}{PCA feature space in 2D plot}{subsubsection.1.8.1}{}}
\newlabel{evident-that-it-maintains-maximum-variance}{{1.8.1}{5}{Evident that it maintains maximum variance}{section*.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Evident that it maintains maximum variance}{5}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Converting inpout space to LDA feature space}{6}{subsection.1.9}\protected@file@percent }
\newlabel{converting-inpout-space-to-lda-feature-space}{{1.9}{6}{Converting inpout space to LDA feature space}{subsection.1.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.1}It is important to note that LDA is not only a dimensionality reducing technique, rather it is also a classificaton technique. It essentially converts the feature space to find the axis that}{6}{subsubsection.1.9.1}\protected@file@percent }
\newlabel{it-is-important-to-note-that-lda-is-not-only-a-dimensionality-reducing-technique-rather-it-is-also-a-classificaton-technique.-it-essentially-converts-the-feature-space-to-find-the-axis-that}{{1.9.1}{6}{It is important to note that LDA is not only a dimensionality reducing technique, rather it is also a classificaton technique. It essentially converts the feature space to find the axis that}{subsubsection.1.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}2D plot of LDA feature space.}{6}{subsection.1.10}\protected@file@percent }
\newlabel{d-plot-of-lda-feature-space.}{{1.10}{6}{2D plot of LDA feature space}{subsection.1.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.1}The difference btween PCA and LDA is evident here. LDA plot has more separability}{6}{subsubsection.1.10.1}\protected@file@percent }
\newlabel{the-difference-btween-pca-and-lda-is-evident-here.-lda-plot-has-more-separability}{{1.10.1}{6}{The difference btween PCA and LDA is evident here. LDA plot has more separability}{subsubsection.1.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11}3D plot of the LDA feature space to inspect separability further}{7}{subsection.1.11}\protected@file@percent }
\newlabel{d-plot-of-the-lda-feature-space-to-inspect-separability-further}{{1.11}{7}{3D plot of the LDA feature space to inspect separability further}{subsection.1.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12}LDA (As a classifier also) tends to overfit the data as it is a supervised technique.}{8}{subsection.1.12}\protected@file@percent }
\newlabel{lda-as-a-classifier-also-tends-to-overfit-the-data-as-it-is-a-supervised-technique.}{{1.12}{8}{LDA (As a classifier also) tends to overfit the data as it is a supervised technique}{subsection.1.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.12.1}It aims to maximise the distance between class means using class labels and this behavior tends to overfit the data if not careful}{8}{subsubsection.1.12.1}\protected@file@percent }
\newlabel{it-aims-to-maximise-the-distance-between-class-means-using-class-labels-and-this-behavior-tends-to-overfit-the-data-if-not-careful}{{1.12.1}{8}{It aims to maximise the distance between class means using class labels and this behavior tends to overfit the data if not careful}{subsubsection.1.12.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.12.2}Here we choose class 0 (Shirt/top) and separate that class into 3 classes.}{8}{subsubsection.1.12.2}\protected@file@percent }
\newlabel{here-we-choose-class-0-shirttop-and-separate-that-class-into-3-classes.}{{1.12.2}{8}{Here we choose class 0 (Shirt/top) and separate that class into 3 classes}{subsubsection.1.12.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13}Visualizing class 0.}{9}{subsection.1.13}\protected@file@percent }
\newlabel{visualizing-class-0.}{{1.13}{9}{Visualizing class 0}{subsection.1.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.13.1}It is the same class for us.}{9}{subsubsection.1.13.1}\protected@file@percent }
\newlabel{it-is-the-same-class-for-us.}{{1.13.1}{9}{It is the same class for us}{subsubsection.1.13.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14}Plotting LDA feature space of constructed pseudo classes.}{10}{subsection.1.14}\protected@file@percent }
\newlabel{plotting-lda-feature-space-of-constructed-pseudo-classes.}{{1.14}{10}{Plotting LDA feature space of constructed pseudo classes}{subsection.1.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.14.1}It is clear to see that the model tries to separate classes even when there is no difference}{10}{subsubsection.1.14.1}\protected@file@percent }
\newlabel{it-is-clear-to-see-that-the-model-tries-to-separate-classes-even-when-there-is-no-difference}{{1.14.1}{10}{It is clear to see that the model tries to separate classes even when there is no difference}{subsubsection.1.14.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.15}Explorng how classification fares on different feature spaces}{11}{subsection.1.15}\protected@file@percent }
\newlabel{explorng-how-classification-fares-on-different-feature-spaces}{{1.15}{11}{Explorng how classification fares on different feature spaces}{subsection.1.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.16}Network trained on Straight up original input space (Best performing model)}{11}{subsection.1.16}\protected@file@percent }
\newlabel{network-trained-on-straight-up-original-input-space-best-performing-model}{{1.16}{11}{Network trained on Straight up original input space (Best performing model)}{subsection.1.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.17}Network trained on PCA input space.}{13}{subsection.1.17}\protected@file@percent }
\newlabel{network-trained-on-pca-input-space.}{{1.17}{13}{Network trained on PCA input space}{subsection.1.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.18}Network trained on LDA feature space}{16}{subsection.1.18}\protected@file@percent }
\newlabel{network-trained-on-lda-feature-space}{{1.18}{16}{Network trained on LDA feature space}{subsection.1.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.19}Using LDA feature space of PCA feature space to train network}{20}{subsection.1.19}\protected@file@percent }
\newlabel{using-lda-feature-space-of-pca-feature-space-to-train-network}{{1.19}{20}{Using LDA feature space of PCA feature space to train network}{subsection.1.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.19.1}Applying PCA to LDA first acts as a regualarizer to LDA and gives better performance than either one. (In this case)}{20}{subsubsection.1.19.1}\protected@file@percent }
\newlabel{applying-pca-to-lda-first-acts-as-a-regualarizer-to-lda-and-gives-better-performance-than-either-one.-in-this-case}{{1.19.1}{20}{Applying PCA to LDA first acts as a regualarizer to LDA and gives better performance than either one. (In this case)}{subsubsection.1.19.1}{}}
